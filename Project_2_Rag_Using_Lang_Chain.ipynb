{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EshaAmjad26/Projects-on-Colab/blob/main/Project_2_Rag_Using_Lang_Chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 2 Rag Using Lang Chain"
      ],
      "metadata": {
        "id": "3leIJ77T5Yzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW6cHV21kgfH"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-pinecone langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyKQyWe7jUQw"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('Gemini-API-Key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hGRkkOTl6WO"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "pcone = Pinecone(api_key = pinecone_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzRlqqfpc6H-",
        "outputId": "b5c4ccec-3148-469c-889f-70cf386f6a47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.04909781739115715,\n",
              " -0.044328317046165466,\n",
              " -0.025365281850099564,\n",
              " -0.030721040442585945,\n",
              " 0.019068587571382523]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "api_key = userdata.get('Gemini-API-Key')\n",
        "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=api_key)\n",
        "vector = embedding_model.embed_query(\"hello world\")\n",
        "vector[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6SZh99lmWuH"
      },
      "outputs": [],
      "source": [
        "index_name = 'online-rag-project-creating'\n",
        "pcone.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "index = pcone.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r95bJOaotF0"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "vector_store = PineconeVectorStore(index= index , embedding = embedding_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agfg5QJKn-hp"
      },
      "outputs": [],
      "source": [
        "#pcone.delete_index(\"online-rag-project\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzqX99WKhGSB"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_core.documents import Document\n",
        "document1 = Document(\n",
        "    page_content = \"LangChain is a framework for developing applications powered by language models. It simplifies integration with LLMs like OpenAI's GPT models.\",\n",
        "    metadata = {\n",
        "            \"title\": \"LangChain Overview\",\n",
        "            \"category\": \"AI Frameworks\",\n",
        "            \"published_date\": \"2025-01-01\"}\n",
        ")\n",
        "\n",
        "document2 = Document(\n",
        "    page_content =\"Pinecone is a vector database that provides fast and scalable storage for embeddings, enabling efficient similarity searches.\",\n",
        "    metadata = {\n",
        "            \"title\": \"Pinecone Introduction\",\n",
        "            \"category\": \"Databases\",\n",
        "            \"published_date\": \"2024-12-15\"}\n",
        ")\n",
        "\n",
        "document3 = Document(\n",
        "    page_content = \"Google Gemini, also known as PaLM 2, is a powerful LLM by Google that excels in multilingual understanding and reasoning tasks.\",\n",
        "    metadata = {\n",
        "            \"title\": \"Google Gemini Overview\",\n",
        "            \"category\": \"Language Models\",\n",
        "            \"published_date\": \"2025-01-05\"}\n",
        ")\n",
        "\n",
        "\n",
        "document = [document1, document2, document3]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCODVq7ClE6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daaffb11-a349-47e0-cf2c-a535651120cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 3\n",
            "Generated IDs: ['doc_1', 'doc_2', 'doc_3']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['doc_1', 'doc_2', 'doc_3']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "doc_id =[ f\"doc_{i+1}\" for i in range(len(document))]\n",
        "print(f\"Number of documents: {len(document)}\")\n",
        "print(f\"Generated IDs: {doc_id}\")\n",
        "\n",
        "vector_store.add_documents(documents=document, ids=doc_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgTTxAvZmgV_",
        "outputId": "076f34a4-c1f3-4455-ed17-b8e95d283594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*LangChain is a framework for developing applications powered by language models. It simplifies integration with LLMs like OpenAI's GPT models. [{'category': 'AI Frameworks', 'published_date': '2025-01-01', 'title': 'LangChain Overview'}]\n",
            "*Pinecone is a vector database that provides fast and scalable storage for embeddings, enabling efficient similarity searches. [{'category': 'Databases', 'published_date': '2024-12-15', 'title': 'Pinecone Introduction'}]\n"
          ]
        }
      ],
      "source": [
        "result = vector_store.similarity_search(\"What is LangChain?\", k=2)\n",
        "result_list = []\n",
        "for res in result:\n",
        "  result_list.append(res.page_content)\n",
        "  print(f\"*{res.page_content} [{res.metadata}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DWhTK_mrItL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a8da749-8518-492c-9003-cc92e188233e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*[SIM : 0.768002] LangChain is a framework for developing applications powered by language models. It simplifies integration with LLMs like OpenAI's GPT models. [{'category': 'AI Frameworks', 'published_date': '2025-01-01', 'title': 'LangChain Overview'}]\n",
            "*[SIM : 0.561469] Pinecone is a vector database that provides fast and scalable storage for embeddings, enabling efficient similarity searches. [{'category': 'Databases', 'published_date': '2024-12-15', 'title': 'Pinecone Introduction'}]\n"
          ]
        }
      ],
      "source": [
        "results = vector_store.similarity_search_with_score(\"What is LangChain?\", k=2)\n",
        "for res, score in results:\n",
        " print(f\"*[SIM : {score:3f}] {res.page_content} [{res.metadata}]\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain_google_genai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "model : ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    model = 'gemini-2.0-flash-exp',\n",
        "    api_key= userdata.get('Gemini-API-Key')\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "T9Y0zu_7lYga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "template = \"\"\"Question : {question},\n",
        "The answer is may be in the following context : {result}\n",
        "Give me the summarized response from this context\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        " # Create the chain (method 1)\n",
        "chain = prompt | model\n"
      ],
      "metadata": {
        "id": "_-6Onpvrn3Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = {'question': 'What is langchain',  'result': result_list[0]}"
      ],
      "metadata": {
        "id": "ia8Y-4kvpTKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke(input)"
      ],
      "metadata": {
        "id": "D3eJq9UCzPwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yMZvAxXMz82e",
        "outputId": "4eea8430-07d8-4dde-ee50-a5bcaa1bdfd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"LangChain is a framework designed to make it easier to build applications that use language models (LLMs) like OpenAI's GPT models.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RGBm9Ejh0DLu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSzxIAKCt4M4PP0OQnDaGi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}